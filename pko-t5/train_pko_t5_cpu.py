#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PKO-T5 ê¸°ë°˜ êµ°ì¤‘ ëª¨ë‹ˆí„°ë§ SLM ëª¨ë¸ í•™ìŠµ ì½”ë“œ (ê°œì„ ëœ ì•ˆì •í™” ë²„ì „)
train_kobart_v2.pyì˜ ì•ˆì •ì ì¸ íŒ¨í„´ì„ ì ìš©
"""

import os
import logging
import pandas as pd
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
    set_seed
)
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
import datetime

# ëª¨ë¸ ìš°ì„ ìˆœìœ„ (ì•ˆì •ì„± ìˆœ)
MODEL_OPTIONS = [
    "paust/pko-t5-base",           # í•œêµ­ì–´ T5 (1ìˆœìœ„)
    "google/flan-t5-base",         # ëŒ€ì•ˆ T5
    "google/mt5-small",            # ë‹¤êµ­ì–´ T5
    "facebook/mbart-large-50"      # ë‹¤êµ­ì–´ BART
]

CSV_FILES = [
    "/Users/yunseong/Desktop/SLM_Model/csv/domain1_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain2_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain3_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain4_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain5_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain6_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain7_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain8_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain9_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain10_dataset.csv",
]
OUTPUT_DIR = "/Volumes/Data/slm_model"
SEED = 42

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'pko_t5_stable_train_log_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_dataset(csv_files):
    """CSV íŒŒì¼ë“¤ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ (ì•ˆì •í™” ë²„ì „)"""
    try:
        all_data = []
        
        for csv_file in csv_files:
            if os.path.exists(csv_file):
                df = pd.read_csv(csv_file, encoding='utf-8')
                all_data.append(df)
                logger.info(f"ë¡œë“œëœ íŒŒì¼: {csv_file}, ë°ì´í„° ìˆ˜: {len(df)}")
            else:
                logger.warning(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {csv_file}")
        
        if len(all_data) == 0:
            raise ValueError("ë¡œë“œëœ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ëª¨ë“  ë°ì´í„° í•©ì¹˜ê¸°
        df = pd.concat(all_data, ignore_index=True)
        logger.info(f"ì›ë³¸ ë°ì´í„° ìˆ˜: {len(df)}")
        
        # ê²°ì¸¡ê°’ ì œê±°
        df = df.dropna(subset=['Domain', 'Input', 'Output'])
        logger.info(f"ì •ì œëœ ë°ì´í„° ìˆ˜: {len(df)}")
        
        # ì•ˆì •ì ì¸ ì…ë ¥ í¬ë§· (train_kobart_v2.py ìŠ¤íƒ€ì¼)
        df['input_text'] = df['Domain'] + ", " + df['Input']
        df['target_text'] = df['Output']
        
        # í†µê³„ ì •ë³´
        logger.info(f"í‰ê·  ì…ë ¥ ê¸¸ì´: {df['input_text'].str.len().mean():.1f}ì")
        logger.info(f"í‰ê·  ì¶œë ¥ ê¸¸ì´: {df['target_text'].str.len().mean():.1f}ì")
        logger.info(f"ë„ë©”ì¸ ì¢…ë¥˜: {df['Domain'].unique().tolist()}")
        
        # ë°ì´í„° ë¶„í• 
        train_df, eval_df = train_test_split(
            df[['input_text', 'target_text']], 
            test_size=0.2, 
            random_state=SEED
        )
        
        # Dataset ê°ì²´ ìƒì„±
        train_dataset = Dataset.from_pandas(train_df)
        eval_dataset = Dataset.from_pandas(eval_df)
        
        return DatasetDict({
            'train': train_dataset,
            'validation': eval_dataset
        })
        
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def preprocess_function_safe(examples, tokenizer, model_type="t5"):
    """ì•ˆì „í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜ (train_kobart_v2.py íŒ¨í„´)"""
    
    inputs = examples["input_text"]
    targets = examples["target_text"]
    
    # T5 ê³„ì—´ ëª¨ë¸ì˜ ê²½ìš° ê°„ë‹¨í•œ prefix ì¶”ê°€
    if "t5" in model_type.lower():
        inputs = ["ë¶„ì„: " + text for text in inputs]
    
    try:
        # ì…ë ¥ í† í°í™” (ë” ì•ˆì „í•œ ì„¤ì •)
        model_inputs = tokenizer(
            inputs,
            max_length=256,
            padding="max_length",
            truncation=True,
            return_tensors=None
        )
        
        # íƒ€ê²Ÿ í† í°í™”
        labels = tokenizer(
            targets,
            max_length=256,
            padding="max_length",
            truncation=True,
            return_tensors=None
        )
        
        # íŒ¨ë”© í† í°ì„ -100ìœ¼ë¡œ ì„¤ì • (í‘œì¤€ ë°©ë²•)
        model_inputs["labels"] = [
            [-100 if token == tokenizer.pad_token_id else token for token in label]
            for label in labels["input_ids"]
        ]
        
        return model_inputs
        
    except Exception as e:
        logger.error(f"ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        # ìµœì†Œí•œì˜ ì•ˆì „í•œ ì „ì²˜ë¦¬
        model_inputs = tokenizer(
            inputs,
            max_length=128,
            padding=True,
            truncation=True,
            return_tensors=None
        )
        
        labels = tokenizer(
            targets,
            max_length=128,
            padding=True,
            truncation=True,
            return_tensors=None
        )
        
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

def try_load_model(model_name):
    """ëª¨ë¸ ë¡œë“œ ì‹œë„ (train_kobart_v2.py íŒ¨í„´)"""
    try:
        logger.info(f"ëª¨ë¸ ë¡œë“œ ì‹œë„: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        
        # íŒ¨ë“œ í† í°ì´ ì—†ìœ¼ë©´ ì¶”ê°€ (íŠ¹ìˆ˜ í† í° ì¶”ê°€ ëŒ€ì‹  ì•ˆì „í•œ ë°©ë²•)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
        return tokenizer, model, model_name
        
    except Exception as e:
        logger.warning(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
        return None, None, None

def test_model_safe(model, tokenizer, device, model_name):
    """ì•ˆì „í•œ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    logger.info("í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
    
    test_inputs = [
        "êµ°ì¤‘ ë°€ì§‘ ë° ì²´ë¥˜ ê°ì§€, 13:00 ê³µì—°ì¥ ì…êµ¬ ì™¸ê³½ ê°ì§€ ì¸ì› 55ëª…",
        "êµ°ì¤‘ ë°€ì§‘ ë° ì²´ë¥˜ ê°ì§€, í˜„ì¬ ì‹œê°ì€ 08ì‹œ 15ë¶„ì´ë©°, ë„ì„œê´€ 2ì¸µ ì •ë¬¸ ì•ì— 18ëª…ì´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¤€ ì¸ì›ì€ 25ëª…ì…ë‹ˆë‹¤."
    ]
    
    # T5 ê³„ì—´ ëª¨ë¸ì˜ ê²½ìš° prefix ì¶”ê°€
    if "t5" in model_name.lower():
        test_inputs = ["ë¶„ì„: " + text for text in test_inputs]
    
    model.eval()
    try:
        for i, input_text in enumerate(test_inputs, 1):
            # í† í°í™”
            inputs = tokenizer(
                input_text, 
                return_tensors="pt", 
                max_length=256, 
                truncation=True,
                padding=True
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ìƒì„±
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=256,
                    num_beams=2,
                    early_stopping=True,
                    no_repeat_ngram_size=2,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”©
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            logger.info(f"í…ŒìŠ¤íŠ¸ {i}:")
            logger.info(f"ì…ë ¥: {input_text}")
            logger.info(f"ì¶œë ¥: {generated_text}")
            logger.info("-" * 50)
            
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì•ˆì •í™” ë²„ì „)"""
    set_seed(SEED)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì • (ì•ˆì •ì„± ìš°ì„ )
    device = "cpu"
    logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ëª¨ë¸ ë¡œë“œ ì‹œë„ (ìš°ì„ ìˆœìœ„ ìˆœ)
    tokenizer, model, model_name = None, None, None
    
    for model_option in MODEL_OPTIONS:
        tokenizer, model, model_name = try_load_model(model_option)
        if model is not None:
            break
    
    if model is None:
        logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
        return False
    
    model = model.to(device)
    
    # í† í¬ë‚˜ì´ì € ì •ë³´
    logger.info(f"ì„ íƒëœ ëª¨ë¸: {model_name}")
    logger.info(f"í† í¬ë‚˜ì´ì € vocab í¬ê¸°: {len(tokenizer)}")
    logger.info(f"íŒ¨ë“œ í† í°: {tokenizer.pad_token}")
    logger.info(f"EOS í† í°: {tokenizer.eos_token}")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    logger.info("ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = load_dataset(CSV_FILES)
    
    # ë°ì´í„° í† í°í™”
    logger.info("ë°ì´í„° í† í°í™” ì¤‘...")
    tokenized_dataset = dataset.map(
        lambda x: preprocess_function_safe(x, tokenizer, model_name),
        batched=True,
        remove_columns=dataset["train"].column_names
    )
    
    # ë°ì´í„° ì½œë ˆì´í„°
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True
    )
    
    # í•™ìŠµ ì¸ìˆ˜ ì„¤ì • (train_kobart_v2.pyì˜ ì•ˆì •ì ì¸ ì„¤ì •)
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        learning_rate=2e-5,  # ì•ˆì „í•œ í•™ìŠµë¥ 
        per_device_train_batch_size=1,  # ì‘ì€ ë°°ì¹˜
        per_device_eval_batch_size=1,
        num_train_epochs=3,  # PKO-T5ì— ì í•©í•œ ì—í¬í¬
        weight_decay=0.01,
        warmup_steps=50,
        logging_steps=20,
        save_steps=100,
        eval_steps=100,
        eval_strategy="steps",
        save_total_limit=2,
        predict_with_generate=False,  # ì•ˆì •ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
        fp16=False,
        gradient_checkpointing=False,
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        push_to_hub=False,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        dataloader_num_workers=0,
        report_to=[],
        # ì¶”ê°€ ì•ˆì •ì„± ì„¤ì •
        max_grad_norm=1.0,
        optim="adamw_torch",
        seed=SEED,
        gradient_accumulation_steps=2  # íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° ì¦ê°€
    )
    
    # íŠ¸ë ˆì´ë„ˆ ì„¤ì • (Seq2SeqTrainer ì‚¬ìš©)
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator
    )
    
    # í•™ìŠµ ì‹œì‘
    logger.info(f"PKO-T5 ëª¨ë¸ í•™ìŠµ ì‹œì‘: {model_name}")
    start_time = datetime.datetime.now()
    
    try:
        # ì²´í¬í¬ì¸íŠ¸ í™•ì¸ ë° ì¬ê°œ í•™ìŠµ ì§€ì›
        resume_from_checkpoint = None
        if os.path.exists(OUTPUT_DIR):
            checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith("checkpoint-")]
            if checkpoints:
                # ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
                latest_checkpoint = max(checkpoints, key=lambda x: int(x.split("-")[1]))
                resume_from_checkpoint = os.path.join(OUTPUT_DIR, latest_checkpoint)
                logger.info(f"ğŸ”„ ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: {resume_from_checkpoint}")
            else:
                logger.info("ğŸ†• ìƒˆë¡œìš´ í•™ìŠµ ì‹œì‘ (ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ)")
        else:
            logger.info("ğŸ†• ìƒˆë¡œìš´ í•™ìŠµ ì‹œì‘ (ì¶œë ¥ ë””ë ‰í† ë¦¬ ì—†ìŒ)")
        
        # í•™ìŠµ ì‹¤í–‰ (ì²´í¬í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ì¬ê°œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ì‹œì‘)
        trainer.train(resume_from_checkpoint=resume_from_checkpoint)
        logger.info("í•™ìŠµ ì™„ë£Œ!")
        
        # ëª¨ë¸ ì €ì¥
        logger.info("ëª¨ë¸ ì €ì¥ ì¤‘...")
        trainer.save_model()
        tokenizer.save_pretrained(OUTPUT_DIR)
        logger.info(f"ëª¨ë¸ì´ {OUTPUT_DIR}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê°„ë‹¨ í…ŒìŠ¤íŠ¸
        test_model_safe(model, tokenizer, device, model_name)
        
        end_time = datetime.datetime.now()
        training_time = end_time - start_time
        logger.info(f"ì´ í•™ìŠµ ì‹œê°„: {training_time}")
        
        return True
        
    except Exception as e:
        logger.error(f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

if __name__ == "__main__":
    logger.info("="*80)
    logger.info("ğŸš€ PKO-T5 êµ°ì¤‘ ëª¨ë‹ˆí„°ë§ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì•ˆì •í™” ë²„ì „)")
    logger.info("="*80)
    
    success = main()
    
    if success:
        logger.info("="*80)
        logger.info("âœ… PKO-T5 ëª¨ë¸ í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info("="*80)
    else:
        logger.info("="*80)
        logger.info("âŒ PKO-T5 ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        logger.info("="*80)