#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PKO-T5-large ê¸°ë°˜ êµ°ì¤‘ ëª¨ë‹ˆí„°ë§ SLM ëª¨ë¸ í•™ìŠµ ì½”ë“œ
ìˆ˜í•™ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•œ ëŒ€ìš©ëŸ‰ ëª¨ë¸ ì‚¬ìš©
"""

import os
import logging
import pandas as pd
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
    set_seed
)
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
import datetime

# ìˆ˜í•™ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•œ ëª¨ë¸ ì˜µì…˜
MODEL_OPTIONS = [
    "paust/pko-t5-large",          # í•œêµ­ì–´ T5 Large (1ìˆœìœ„ - ìˆ˜í•™ ì¶”ë¡  ìš°ìˆ˜)
    "google/flan-t5-large",         # Flan-T5 Large (ìˆ˜í•™ ì¶”ë¡  íŠ¹í™”)
    "google/mt5-large",             # ë‹¤êµ­ì–´ T5 Large
    "paust/pko-t5-base",            # í•œêµ­ì–´ T5 Base (ëŒ€ì•ˆ)
]

CSV_FILES = [
    "/Users/yunseong/Desktop/SLM_Model/csv/domain1_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain2_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain3_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain4_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain5_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain6_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain7_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain8_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain9_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain10_dataset.csv",
]

OUTPUT_DIR = "/Volumes/Data/slm_model_large"  # Large ëª¨ë¸ìš© ë³„ë„ ë””ë ‰í† ë¦¬
SEED = 42

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'pko_t5_large_train_log_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_dataset_with_math_focus(csv_files):
    """ìˆ˜í•™ ê³„ì‚°ì´ í¬í•¨ëœ ë°ì´í„°ì…‹ ë¡œë“œ (ìˆ˜í•™ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒìš©)"""
    try:
        all_data = []
        
        for csv_file in csv_files:
            if os.path.exists(csv_file):
                df = pd.read_csv(csv_file, encoding='utf-8')
                all_data.append(df)
                logger.info(f"ë¡œë“œëœ íŒŒì¼: {csv_file}, ë°ì´í„° ìˆ˜: {len(df)}")
            else:
                logger.warning(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {csv_file}")
        
        if len(all_data) == 0:
            raise ValueError("ë¡œë“œëœ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ëª¨ë“  ë°ì´í„° í•©ì¹˜ê¸°
        df = pd.concat(all_data, ignore_index=True)
        logger.info(f"ì›ë³¸ ë°ì´í„° ìˆ˜: {len(df)}")
        
        # ê²°ì¸¡ê°’ ì œê±°
        df = df.dropna(subset=['Domain', 'Input', 'Output'])
        logger.info(f"ì •ì œëœ ë°ì´í„° ìˆ˜: {len(df)}")
        
        # ìˆ˜í•™ ê³„ì‚°ì´ í¬í•¨ëœ ë°ì´í„° í•„í„°ë§ (ì„ íƒì )
        math_keywords = ['ëª…', 'ê¸°ì¤€', 'ìˆ˜ìš©ì¸ì›', 'í—ˆìš©', 'ìµœëŒ€', 'ì œí•œ', 'ì´ˆê³¼', 'ë¶€ì¡±', 'ì°¨ì´']
        math_data = df[df['Input'].str.contains('|'.join(math_keywords), na=False)]
        logger.info(f"ìˆ˜í•™ ê³„ì‚° ë°ì´í„° ìˆ˜: {len(math_data)}")
        
        # ìˆ˜í•™ ê³„ì‚° ë°ì´í„°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í¬í•¨
        if len(math_data) > 0:
            # ìˆ˜í•™ ë°ì´í„° 70%, ì¼ë°˜ ë°ì´í„° 30% ë¹„ìœ¨ë¡œ ì¡°ì •
            math_ratio = min(0.7, len(math_data) / len(df))
            math_sample_size = int(len(df) * math_ratio)
            general_sample_size = len(df) - math_sample_size
            
            # ìˆ˜í•™ ë°ì´í„° ìƒ˜í”Œë§
            math_sample = math_data.sample(n=min(math_sample_size, len(math_data)), random_state=SEED)
            
            # ì¼ë°˜ ë°ì´í„°ì—ì„œ ìˆ˜í•™ ë°ì´í„° ì œì™¸ í›„ ìƒ˜í”Œë§
            general_data = df[~df.index.isin(math_data.index)]
            general_sample = general_data.sample(n=min(general_sample_size, len(general_data)), random_state=SEED)
            
            # ë°ì´í„° í•©ì¹˜ê¸°
            df = pd.concat([math_sample, general_sample], ignore_index=True)
            logger.info(f"ìˆ˜í•™ ê³„ì‚° ì¤‘ì‹¬ ë°ì´í„°ì…‹: {len(df)}ê°œ")
        
        # ìˆ˜í•™ ì¶”ë¡ ì„ ìœ„í•œ ì…ë ¥ í¬ë§· ê°•í™”
        df['input_text'] = "ìˆ˜í•™ ë¶„ì„: " + df['Domain'] + ", " + df['Input']
        df['target_text'] = df['Output']
        
        # í†µê³„ ì •ë³´
        logger.info(f"í‰ê·  ì…ë ¥ ê¸¸ì´: {df['input_text'].str.len().mean():.1f}ì")
        logger.info(f"í‰ê·  ì¶œë ¥ ê¸¸ì´: {df['target_text'].str.len().mean():.1f}ì")
        logger.info(f"ë„ë©”ì¸ ì¢…ë¥˜: {df['Domain'].unique().tolist()}")
        
        # ë°ì´í„° ë¶„í• 
        train_df, eval_df = train_test_split(
            df[['input_text', 'target_text']], 
            test_size=0.2, 
            random_state=SEED
        )
        
        # Dataset ê°ì²´ ìƒì„±
        train_dataset = Dataset.from_pandas(train_df)
        eval_dataset = Dataset.from_pandas(eval_df)
        
        return DatasetDict({
            'train': train_dataset,
            'validation': eval_dataset
        })
        
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def preprocess_function_math_focused(examples, tokenizer, model_type="t5"):
    """ìˆ˜í•™ ì¶”ë¡ ì— íŠ¹í™”ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        inputs = examples["input_text"]
        targets = examples["target_text"]
        
        # T5 ê³„ì—´ ëª¨ë¸ì˜ ê²½ìš° ìˆ˜í•™ ë¶„ì„ prefix ì¶”ê°€
        if "t5" in model_type.lower():
            inputs = ["ìˆ˜í•™ ë¶„ì„: " + text.replace("ìˆ˜í•™ ë¶„ì„: ", "") for text in inputs]
        
        # ì…ë ¥ í† í°í™” (ìˆ˜í•™ ì¶”ë¡ ì„ ìœ„í•´ ë” ê¸´ ì‹œí€€ìŠ¤ í—ˆìš©)
        model_inputs = tokenizer(
            inputs,
            max_length=512,  # ìˆ˜í•™ ê³„ì‚°ì„ ìœ„í•´ ë” ê¸´ ì‹œí€€ìŠ¤
            padding="max_length",
            truncation=True,
            return_tensors=None
        )
        
        # íƒ€ê²Ÿ í† í°í™”
        labels = tokenizer(
            targets,
            max_length=512,  # ìˆ˜í•™ ê³„ì‚° ê²°ê³¼ë¥¼ ìœ„í•´ ë” ê¸´ ì‹œí€€ìŠ¤
            padding="max_length",
            truncation=True,
            return_tensors=None
        )
        
        # íŒ¨ë”© í† í°ì„ -100ìœ¼ë¡œ ì„¤ì •
        model_inputs["labels"] = [
            [-100 if token == tokenizer.pad_token_id else token for token in label]
            for label in labels["input_ids"]
        ]
        
        return model_inputs
        
    except Exception as e:
        logger.error(f"ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        raise

def try_load_large_model():
    """Large ëª¨ë¸ ë¡œë“œ ì‹œë„"""
    for model in MODEL_OPTIONS:
        try:
            logger.info(f"Large ëª¨ë¸ ë¡œë“œ ì‹œë„: {model}")
            tokenizer = AutoTokenizer.from_pretrained(model)
            model_obj = AutoModelForSeq2SeqLM.from_pretrained(model)
            
            # í† í¬ë‚˜ì´ì € ì„¤ì •
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            logger.info(f"âœ… Large ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model}")
            logger.info(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model_obj.parameters()):,}")
            return model_obj, tokenizer, model
            
        except Exception as e:
            logger.warning(f"âŒ Large ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model}, ì˜¤ë¥˜: {e}")
            continue
    
    raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ Large ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def test_math_reasoning(model, tokenizer, device, model_name):
    """ìˆ˜í•™ ì¶”ë¡  ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§® ìˆ˜í•™ ì¶”ë¡  ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸...")
    
    math_test_cases = [
        "êµ°ì¤‘ ë°€ì§‘ ë° ì²´ë¥˜ ê°ì§€, ë†êµ¬ê²½ê¸°ì¥ ê³„ë‹¨ì— 12:14 í˜„ì¬ 4ëª…ì´ ìˆìœ¼ë©°, ìš´ì˜ ê¸°ì¤€ì€ 9ëª…ì…ë‹ˆë‹¤.",
        "êµ°ì¤‘ ë°€ì§‘ ë° ì²´ë¥˜ ê°ì§€, 13:13 í´ë¦¬ë‹‰ì—ì„œ 71ëª… ë°€ì§‘, ê¸°ì¤€ ìˆ˜ìš©ì¸ì› 49ëª…",
        "êµ°ì¤‘ ë°€ì§‘ ë° ì²´ë¥˜ ê°ì§€, 8:29 êµ¬ì²­ 309ëª… ê³„ì¸¡, ê¸°ì¤€ ì¸ì› 401ëª…",
        "êµ°ì¤‘ ë°€ì§‘ ë° ì²´ë¥˜ ê°ì§€, 20:32 ë°±í™”ì  47ëª… íŒŒì•…, ê¸°ì¤€ 39ëª…",
        "êµ°ì¤‘ ë°€ì§‘ ë° ì²´ë¥˜ ê°ì§€, ì˜¤ì „ 11ì‹œ 10ë¶„ ê·¹ì¥ 80ëª… ê³„ì¸¡, ê¸°ì¤€ ì¸ì› 63ëª…"
    ]
    
    # T5 ê³„ì—´ ëª¨ë¸ì˜ ê²½ìš° prefix ì¶”ê°€
    if "t5" in model_name.lower():
        math_test_cases = ["ìˆ˜í•™ ë¶„ì„: " + text for text in math_test_cases]
    
    model.eval()
    try:
        for i, input_text in enumerate(math_test_cases, 1):
            # í† í°í™”
            inputs = tokenizer(
                input_text, 
                return_tensors="pt", 
                max_length=512, 
                truncation=True,
                padding=True
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ìƒì„± (ìˆ˜í•™ ì¶”ë¡ ì„ ìœ„í•´ ë” ì •êµí•œ ì„¤ì •)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=512,
                    num_beams=4,  # ë” ì •í™•í•œ ìƒì„±ì„ ìœ„í•´ ë¹” ì„œì¹˜ ì¦ê°€
                    early_stopping=True,
                    no_repeat_ngram_size=3,
                    do_sample=True,
                    temperature=0.7,  # ì°½ì˜ì„±ê³¼ ì •í™•ì„±ì˜ ê· í˜•
                    top_p=0.9,
                    repetition_penalty=1.1,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”©
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            logger.info(f"ìˆ˜í•™ í…ŒìŠ¤íŠ¸ {i}:")
            logger.info(f"ì…ë ¥: {input_text}")
            logger.info(f"ì¶œë ¥: {generated_text}")
            logger.info("-" * 50)
            
    except Exception as e:
        logger.error(f"ìˆ˜í•™ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (Large ëª¨ë¸ ìˆ˜í•™ ì¶”ë¡  íŠ¹í™”)"""
    set_seed(SEED)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì • (Large ëª¨ë¸ì€ GPU ê¶Œì¥)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # Large ëª¨ë¸ ë¡œë“œ
    logger.info("ğŸ¤– Large ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model, tokenizer, model_name = try_load_large_model()
    model = model.to(device)
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    logger.info(f"ì„ íƒëœ Large ëª¨ë¸: {model_name}")
    logger.info(f"í† í¬ë‚˜ì´ì € vocab í¬ê¸°: {len(tokenizer)}")
    logger.info(f"íŒ¨ë“œ í† í°: {tokenizer.pad_token}")
    
    # ìˆ˜í•™ ì¶”ë¡  ì¤‘ì‹¬ ë°ì´í„°ì…‹ ë¡œë“œ
    logger.info("ğŸ“Š ìˆ˜í•™ ì¶”ë¡  ì¤‘ì‹¬ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = load_dataset_with_math_focus(CSV_FILES)
    logger.info(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: í›ˆë ¨ {len(dataset['train'])}ê°œ, ê²€ì¦ {len(dataset['validation'])}ê°œ")
    
    # ë°ì´í„° í† í°í™”
    logger.info("ğŸ”„ ë°ì´í„° í† í°í™” ì¤‘...")
    tokenized_dataset = dataset.map(
        lambda x: preprocess_function_math_focused(x, tokenizer, model_name),
        batched=True,
        remove_columns=dataset["train"].column_names
    )
    
    # ë°ì´í„° ì½œë ˆì´í„°
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True
    )
    
    # Large ëª¨ë¸ì— ìµœì í™”ëœ í•™ìŠµ ì„¤ì •
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        learning_rate=1e-5,  # Large ëª¨ë¸ì— ì í•©í•œ ë‚®ì€ í•™ìŠµë¥ 
        per_device_train_batch_size=2,  # Large ëª¨ë¸ ë©”ëª¨ë¦¬ ì œì•½
        per_device_eval_batch_size=2,
        num_train_epochs=2,  # Large ëª¨ë¸ì€ ì ì€ ì—í¬í¬ë¡œë„ ì¶©ë¶„
        weight_decay=0.01,
        warmup_steps=100,
        logging_steps=50,
        save_steps=200,
        eval_steps=200,
        eval_strategy="steps",
        save_total_limit=2,
        predict_with_generate=True,
        fp16=True if device.type == "cuda" else False,  # GPU ì‚¬ìš©ì‹œ í˜¼í•© ì •ë°€ë„
        gradient_checkpointing=True,  # ë©”ëª¨ë¦¬ ì ˆì•½
        dataloader_pin_memory=False,  # GPU ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œ í•´ê²°
        remove_unused_columns=False,
        push_to_hub=False,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        dataloader_num_workers=0,  # GPU ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œ í•´ê²°
        report_to=[],
        # Large ëª¨ë¸ ìµœì í™” ì„¤ì •
        max_grad_norm=1.0,
        optim="adamw_torch",
        seed=SEED,
        gradient_accumulation_steps=4,  # íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° ì¦ê°€
        # ìˆ˜í•™ ì¶”ë¡ ì„ ìœ„í•œ ì¶”ê°€ ì„¤ì •
        generation_max_length=512,
        generation_num_beams=4
    )
    
    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator
    )
    
    # í•™ìŠµ ì‹œì‘
    logger.info(f"ğŸš€ PKO-T5-Large ëª¨ë¸ ìˆ˜í•™ ì¶”ë¡  í•™ìŠµ ì‹œì‘: {model_name}")
    start_time = datetime.datetime.now()
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ ê°€ëŠ¥
    resume_from_checkpoint = None
    if os.path.exists(OUTPUT_DIR):
        checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith("checkpoint-")]
        if checkpoints:
            latest = max(checkpoints, key=lambda x: int(x.split("-")[1]))
            resume_from_checkpoint = os.path.join(OUTPUT_DIR, latest)
            logger.info(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: {resume_from_checkpoint}")
    
    trainer.train(resume_from_checkpoint=resume_from_checkpoint)
    
    # ëª¨ë¸ ì €ì¥
    logger.info("ğŸ’¾ Large ëª¨ë¸ ì €ì¥ ì¤‘...")
    trainer.save_model()
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    # ìˆ˜í•™ ì¶”ë¡  ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸
    logger.info("ğŸ§® í•™ìŠµëœ Large ëª¨ë¸ ìˆ˜í•™ ì¶”ë¡  í…ŒìŠ¤íŠ¸...")
    test_math_reasoning(model, tokenizer, device, model_name)
    
    elapsed = datetime.datetime.now() - start_time
    logger.info(f"âœ… Large ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ì´ ì‹œê°„: {elapsed}")
    
    return True

if __name__ == "__main__":
    main()