#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PKO-T5 ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ SLM ëª¨ë¸ í•™ìŠµ ì½”ë“œ
ê¸°ì¡´ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì— í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í†µí•©
"""

import os
import logging
import pandas as pd
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
    set_seed
)
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
import datetime
import sys

# ìƒìœ„ ë””ë ‰í† ë¦¬ ì¶”ê°€ (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“ˆ importë¥¼ ìœ„í•´)
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“ˆ import
try:
    from math_calculator import MathCalculator
    from hybrid_slm_generator import HybridSLMGenerator
    HYBRID_AVAILABLE = True
except ImportError:
    HYBRID_AVAILABLE = False
    print("âš ï¸ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

# ëª¨ë¸ ìš°ì„ ìˆœìœ„ (ì•ˆì •ì„± ìˆœ)
MODEL_OPTIONS = [
    "paust/pko-t5-base",           # í•œêµ­ì–´ T5 (1ìˆœìœ„)
    "google/flan-t5-base",         # ëŒ€ì•ˆ T5
    "google/mt5-small",            # ë‹¤êµ­ì–´ T5
    "facebook/mbart-large-50"      # ë‹¤êµ­ì–´ BART
]

CSV_FILES = [
    "/Users/yunseong/Desktop/SLM_Model/csv/domain1_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain2_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain3_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain4_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain5_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain6_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain7_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain8_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain9_dataset.csv",
    "/Users/yunseong/Desktop/SLM_Model/csv/domain10_dataset.csv",
]
OUTPUT_DIR = "/Volumes/Data/slm_model"
SEED = 42

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'pko_t5_hybrid_train_log_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_dataset(csv_files):
    """CSV íŒŒì¼ë“¤ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ (í•˜ì´ë¸Œë¦¬ë“œ ì§€ì›)"""
    try:
        all_data = []
        
        for csv_file in csv_files:
            if os.path.exists(csv_file):
                df = pd.read_csv(csv_file, encoding='utf-8')
                all_data.append(df)
                logger.info(f"ë¡œë“œëœ íŒŒì¼: {csv_file}, ë°ì´í„° ìˆ˜: {len(df)}")
            else:
                logger.warning(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {csv_file}")
        
        if len(all_data) == 0:
            raise ValueError("ë¡œë“œëœ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ëª¨ë“  ë°ì´í„° í•©ì¹˜ê¸°
        df = pd.concat(all_data, ignore_index=True)
        logger.info(f"ì›ë³¸ ë°ì´í„° ìˆ˜: {len(df)}")
        
        # ê²°ì¸¡ê°’ ì œê±°
        df = df.dropna(subset=['Domain', 'Input', 'Output'])
        logger.info(f"ì •ì œëœ ë°ì´í„° ìˆ˜: {len(df)}")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì„ ìœ„í•œ ì…ë ¥ í¬ë§·
        df['input_text'] = "ë¶„ì„: " + df['Domain'] + ", " + df['Input']
        df['target_text'] = df['Output']
        
        # í†µê³„ ì •ë³´
        logger.info(f"í‰ê·  ì…ë ¥ ê¸¸ì´: {df['input_text'].str.len().mean():.1f}ì")
        logger.info(f"í‰ê·  ì¶œë ¥ ê¸¸ì´: {df['target_text'].str.len().mean():.1f}ì")
        logger.info(f"ë„ë©”ì¸ ì¢…ë¥˜: {df['Domain'].unique().tolist()}")
        
        # ë°ì´í„° ë¶„í• 
        train_df, eval_df = train_test_split(
            df[['input_text', 'target_text']], 
            test_size=0.2, 
            random_state=SEED
        )
        
        # Dataset ê°ì²´ ìƒì„±
        train_dataset = Dataset.from_pandas(train_df)
        eval_dataset = Dataset.from_pandas(eval_df)
        
        return DatasetDict({
            'train': train_dataset,
            'validation': eval_dataset
        })
        
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def preprocess_function_safe(examples, tokenizer, model_type="t5"):
    """ì•ˆì „í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜ (í•˜ì´ë¸Œë¦¬ë“œ ì§€ì›)"""
    try:
        # ì…ë ¥ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        inputs = examples['input_text']
        targets = examples['target_text']
        
        # í† í¬ë‚˜ì´ì € ì„¤ì •
        model_inputs = tokenizer(
            inputs,
            max_length=256,
            padding='max_length',
            truncation=True,
            return_tensors="pt"
        )
        
        # íƒ€ê²Ÿ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        with tokenizer.as_target_tokenizer():
            labels = tokenizer(
                targets,
                max_length=256,
                padding='max_length',
                truncation=True,
                return_tensors="pt"
            )
        
        # íŒ¨ë”© í† í°ì„ -100ìœ¼ë¡œ ì„¤ì • (loss ê³„ì‚°ì—ì„œ ì œì™¸)
        labels["input_ids"][labels["input_ids"] == tokenizer.pad_token_id] = -100
        
        model_inputs["labels"] = labels["input_ids"]
        
        return model_inputs
        
    except Exception as e:
        logger.error(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

def try_load_model(model_name):
    """ëª¨ë¸ ë¡œë“œ ì‹œë„"""
    for model in MODEL_OPTIONS:
        try:
            logger.info(f"ëª¨ë¸ ë¡œë“œ ì‹œë„: {model}")
            tokenizer = AutoTokenizer.from_pretrained(model)
            model_obj = AutoModelForSeq2SeqLM.from_pretrained(model)
            
            # í† í¬ë‚˜ì´ì € ì„¤ì •
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model}")
            return model_obj, tokenizer
            
        except Exception as e:
            logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model}, ì˜¤ë¥˜: {e}")
            continue
    
    raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def test_hybrid_system(model_path, test_cases=None):
    """í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    if not HYBRID_AVAILABLE:
        logger.warning("í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        logger.info("ğŸ§ª í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¤€ë¹„
        if test_cases is None:
            test_cases = [
                "ë†êµ¬ê²½ê¸°ì¥ ê³„ë‹¨ì— 12:14 í˜„ì¬ 4ëª…ì´ ìˆìœ¼ë©°, ìš´ì˜ ê¸°ì¤€ì€ 9ëª…ì…ë‹ˆë‹¤.",
                "13:13 í´ë¦¬ë‹‰ì—ì„œ 71ëª… ë°€ì§‘, ê¸°ì¤€ ìˆ˜ìš©ì¸ì› 49ëª…",
                "8:29 êµ¬ì²­ 309ëª… ê³„ì¸¡, ê¸°ì¤€ ì¸ì› 401ëª…"
            ]
        
        # í•˜ì´ë¸Œë¦¬ë“œ ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = HybridSLMGenerator(model_path)
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = []
        for i, test_input in enumerate(test_cases, 1):
            logger.info(f"í…ŒìŠ¤íŠ¸ {i}: {test_input}")
            
            result = generator.analyze_generation(test_input)
            results.append(result)
            
            if result["calculation"]:
                logger.info(f"  ê³„ì‚°: {result['calculation']['description']}")
                if result["improved"]:
                    logger.info("  âœ… ê°œì„ ë¨")
                elif result["hybrid_correct"]:
                    logger.info("  âœ… ì •í™•í•¨")
                else:
                    logger.warning("  âŒ ì—¬ì „íˆ ì˜¤ë¥˜")
        
        # ê²°ê³¼ ìš”ì•½
        total = len(results)
        improved = sum(1 for r in results if r["improved"])
        correct = sum(1 for r in results if r["hybrid_correct"])
        
        logger.info(f"ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        logger.info(f"ì´ í…ŒìŠ¤íŠ¸: {total}ê°œ")
        logger.info(f"ê°œì„ ëœ ì¼€ì´ìŠ¤: {improved}ê°œ ({improved/total*100:.1f}%)")
        logger.info(f"ì •í™•í•œ ê³„ì‚°: {correct}ê°œ ({correct/total*100:.1f}%)")
        
        return results
        
    except Exception as e:
        logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return None

def main():
    """ë©”ì¸ í•¨ìˆ˜ (í•˜ì´ë¸Œë¦¬ë“œ ì§€ì›)"""
    try:
        logger.info("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ SLM ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        
        # ì‹œë“œ ì„¤ì •
        set_seed(SEED)
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        logger.info("ğŸ“Š ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
        dataset = load_dataset(CSV_FILES)
        logger.info(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: í›ˆë ¨ {len(dataset['train'])}ê°œ, ê²€ì¦ {len(dataset['validation'])}ê°œ")
        
        # ëª¨ë¸ ë¡œë“œ
        logger.info("ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
        model, tokenizer = try_load_model(MODEL_OPTIONS[0])
        
        # ì „ì²˜ë¦¬ í•¨ìˆ˜ ì„¤ì •
        def preprocess_function(examples):
            return preprocess_function_safe(examples, tokenizer)
        
        # ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
        logger.info("ğŸ”„ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì¤‘...")
        tokenized_datasets = dataset.map(
            preprocess_function,
            batched=True,
            remove_columns=dataset["train"].column_names
        )
        
        # í•™ìŠµ ì„¤ì •
        training_args = Seq2SeqTrainingArguments(
            output_dir=OUTPUT_DIR,
            evaluation_strategy="steps",
            eval_steps=500,
            save_strategy="steps",
            save_steps=500,
            learning_rate=5e-5,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            weight_decay=0.01,
            save_total_limit=3,
            num_train_epochs=3,
            predict_with_generate=True,
            fp16=False,  # CPU í•™ìŠµ
            dataloader_num_workers=4,
            logging_steps=100,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to=None,  # wandb ë¹„í™œì„±í™”
        )
        
        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForSeq2Seq(
            tokenizer,
            model=model,
            label_pad_token_id=-100,
            pad_to_multiple_of=8
        )
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = Seq2SeqTrainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_datasets["train"],
            eval_dataset=tokenized_datasets["validation"],
            tokenizer=tokenizer,
            data_collator=data_collator,
        )
        
        # ëª¨ë¸ í•™ìŠµ
        logger.info("ğŸ¯ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        logger.info("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        trainer.save_model()
        tokenizer.save_pretrained(OUTPUT_DIR)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        if HYBRID_AVAILABLE:
            logger.info("ğŸ§ª í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
            test_hybrid_system(OUTPUT_DIR)
        else:
            logger.warning("í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"í•™ìŠµ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    main() 