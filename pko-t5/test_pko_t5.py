#!/usr/bin/env python3
"""
í•œêµ­ì–´ T5 ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
MPS ì˜¤ë¥˜ ë°©ì§€ ë° ì•ˆì „í•œ í…ŒìŠ¤íŠ¸
"""

import os
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì„¤ì •
MODEL_DIR = "./test1"
CSV_PATH = "../csv/domain1_example_dataset.csv"

def load_model_safe():
    """ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ"""
    try:
        # CPU ê°•ì œ ì‚¬ìš© (MPS ì˜¤ë¥˜ ë°©ì§€)
        device = "cpu"
        logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)
        model = model.to(device)
        
        logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        return model, tokenizer, device
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None, None

def generate_text_safe(model, tokenizer, device, input_text):
    """ì•ˆì „í•œ í…ìŠ¤íŠ¸ ìƒì„±"""
    try:
        # T5 ëª¨ë¸ìš© prefix ì¶”ê°€
        input_with_prefix = "ë¶„ì„: " + input_text
        
        # í† í°í™”
        inputs = tokenizer(
            input_with_prefix,
            return_tensors="pt",
            max_length=256,
            truncation=True,
            padding=True
        )
        
        # CPUë¡œ ì´ë™
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # ìƒì„± (ì•ˆì „í•œ ì„¤ì •)
        model.eval()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=512,
                num_beams=3,
                early_stopping=True,
                no_repeat_ngram_size=2,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                temperature=1.0
            )
        
        # ë””ì½”ë”©
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return generated_text
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return f"ì˜¤ë¥˜: {str(e)}"

def calculate_similarity(text1, text2):
    """ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚°"""
    words1 = set(text1.split())
    words2 = set(text2.split())
    
    if not words1 and not words2:
        return 1.0
    if not words1 or not words2:
        return 0.0
        
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    return len(intersection) / len(union)

def test_model():
    """ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("="*80)
    logger.info("ğŸ” í•œêµ­ì–´ T5 ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    logger.info("="*80)
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer, device = load_model_safe()
    if model is None:
        logger.error("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨!")
        return
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    try:
        df = pd.read_csv(CSV_PATH, encoding='utf-8')
        logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜: {len(df)}")
    except Exception as e:
        logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (ì²˜ìŒ 10ê°œ)
    test_samples = df.head(10)
    total_similarity = 0
    successful_tests = 0
    
    for idx, row in test_samples.iterrows():
        try:
            domain = row['Domain']
            input_text = row['Input']
            expected_output = row['Output']
            
            # ì…ë ¥ ìƒì„± (ë„ë©”ì¸ + ì…ë ¥)
            combined_input = f"{domain}, {input_text}"
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            generated_output = generate_text_safe(model, tokenizer, device, combined_input)
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarity = calculate_similarity(expected_output, generated_output)
            total_similarity += similarity
            successful_tests += 1
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“ ìƒ˜í”Œ {idx + 1}:")
            print("-" * 70)
            print(f"ë„ë©”ì¸: {domain}")
            print(f"ì…ë ¥: {input_text}")
            print(f"ê¸°ëŒ€ ì¶œë ¥: {expected_output}")
            print(f"ì˜ˆì¸¡ ì¶œë ¥: {generated_output}")
            print(f"ìœ ì‚¬ë„ ì ìˆ˜: {similarity:.3f}")
            print(f"ê¸¸ì´ - ì…ë ¥: {len(input_text)}, ê¸°ëŒ€: {len(expected_output)}, ì˜ˆì¸¡: {len(generated_output)}")
            
        except Exception as e:
            logger.error(f"ìƒ˜í”Œ {idx + 1} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            continue
    
    # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
    if successful_tests > 0:
        avg_similarity = total_similarity / successful_tests
        print(f"\nğŸ¯ ì „ì²´ ì„±ëŠ¥ ìš”ì•½:")
        print(f"í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.3f}")
        print(f"ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {successful_tests}/{len(test_samples)}")
        
        # ì„±ëŠ¥ í‰ê°€
        if avg_similarity >= 0.7:
            print("ğŸ† ìš°ìˆ˜í•œ ì„±ëŠ¥!")
        elif avg_similarity >= 0.5:
            print("âœ… ì–‘í˜¸í•œ ì„±ëŠ¥!")
        elif avg_similarity >= 0.3:
            print("âš ï¸ ë³´í†µ ì„±ëŠ¥ - ê°œì„  í•„ìš”")
        else:
            print("âŒ ë‚®ì€ ì„±ëŠ¥ - ì¶”ê°€ í•™ìŠµ í•„ìš”")
    
    # ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸
    print(f"\nğŸ—£ï¸ ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
    print("-" * 70)
    
    while True:
        try:
            user_input = input("\nì…ë ¥ í…ìŠ¤íŠ¸: ").strip()
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                break
            
            if not user_input:
                continue
            
            # ë„ë©”ì¸ ìë™ ì¶”ê°€
            full_input = f"êµ°ì¤‘ ë°€ì§‘ ë° ì²´ë¥˜ ê°ì§€, {user_input}"
            generated = generate_text_safe(model, tokenizer, device, full_input)
            
            print(f"ìƒì„±ëœ ë¶„ì„: {generated}")
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"ì˜¤ë¥˜: {e}")
    
    print("\ní…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    test_model()